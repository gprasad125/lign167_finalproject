{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gprasad125/lign167_finalproject/blob/main/Copy_of_Project_Tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f7ef64e",
      "metadata": {
        "id": "5f7ef64e"
      },
      "source": [
        "# Using Multiclass Text Classification to Analyze Famous Quotes \n",
        "\n",
        "#### Gokul Prasad & Hoang Nguyen \n",
        "#### LIGN 167, Winter 2022"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8998fd17",
      "metadata": {
        "id": "8998fd17"
      },
      "source": [
        "In this project, we'll aim to classify a variety of quotes with tags that refer to certain themes or elements specific to that particular quote. \n",
        "\n",
        "For example, Albert Einstein's quote “Life is like riding a bicycle. To keep your balance, you must keep moving.” would have tags like \"life\" or \"simile\" because it contains thematic elements about life, and contains a simile. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1773b1c2",
      "metadata": {
        "id": "1773b1c2"
      },
      "outputs": [],
      "source": [
        "# Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time \n",
        "\n",
        "# Data manipulation / cleaning / visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim as gm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import re \n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "# Sklearn modeling\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras modeling\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "from keras.metrics import Precision, Recall\n",
        "\n",
        "# Transformers for model 2.2\n",
        "import transformers\n",
        "from transformers import AutoTokenizer,TFBertModel\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2b1c8b6",
      "metadata": {
        "id": "f2b1c8b6"
      },
      "source": [
        "# Scraping and Cleaning the Data "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5efa41f",
      "metadata": {
        "id": "f5efa41f"
      },
      "source": [
        "We'll be sourcing our data from https://www.goodreads.com/quotes. This is a website containing 100 pages worth of quotes, each of them classified with a few tags. \n",
        "\n",
        "Firstly, we'll loop through the pages, and scrape the website HTML data with BeautifulSoup. Then, we'll use lambda functions to pull author data, quote data, and tag data. We'll put each of these into lists, and then create a pandas DataFrame to hold all our data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9738cafb",
      "metadata": {
        "id": "9738cafb"
      },
      "outputs": [],
      "source": [
        "goodreads_quotes = []\n",
        "goodreads_tags = []\n",
        "\n",
        "for i in range(1, 101):\n",
        "\n",
        "  url = 'https://www.goodreads.com/quotes?page={}'.format(i)\n",
        "\n",
        "  time.sleep(5)\n",
        "  scrape = requests.get(url)\n",
        "  parsed = BeautifulSoup(scrape.content, 'html.parser')\n",
        "\n",
        "  elements_quotes = parsed.find_all('div', class_ = \"quoteText\")\n",
        "  \n",
        "  quotes = [x.text.strip() for x in elements_quotes]\n",
        "  tags = parsed.find_all(class_ = 'quoteFooter')\n",
        "\n",
        "  goodreads_quotes += quotes\n",
        "  goodreads_tags += tags"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'quote':goodreads_quotes, 'tags':goodreads_tags}\n",
        "goodreads = pd.DataFrame(data)\n",
        "goodreads.head()"
      ],
      "metadata": {
        "id": "jZMT3V2z8r-u"
      },
      "id": "jZMT3V2z8r-u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "289aba52",
      "metadata": {
        "id": "289aba52"
      },
      "source": [
        "As we can see, our dataset contains some pretty messy strings in both columns. We'll need to process the data to make sure it's usable for our modeling later on. \n",
        "\n",
        "For quotes, we'll first make all characters lowercase, and then use regex functionality to substitute any non alphanumeric / whitespace character with a blank string. \n",
        "\n",
        "For example, if we input a quote like \"I love. LIGN \\n167!!?   \\n \\nOscar Wilde \" we would receive an output of \"i love lign 167\". We'll apply this to our Author and Quote columns to clean them up and make them much more simplified strings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bbad6f0",
      "metadata": {
        "id": "6bbad6f0"
      },
      "outputs": [],
      "source": [
        "def quotes_cleaning(text):\n",
        "    \n",
        "    text = text.lower()\n",
        "    \n",
        "    text = re.sub('[^A-Za-z0-9\\s]', '', text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def tags_cleaning(text):\n",
        "    \n",
        "    text = re.sub('[\\[ \\]]', ' ', str(text))\n",
        "    text = re.sub('[^\\w]', ' ', text)\n",
        "    text = re.sub('[\\s]', ' ', text)\n",
        "    text = re.sub('[0-9]', ' ', text)\n",
        "    \n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text.split(' ')\n",
        "\n",
        "def remove_author(quote):\n",
        "\n",
        "  if quote[0] == '“':\n",
        "\n",
        "    end_of_quote = quote.index('”')\n",
        "    quote = quote[1:end_of_quote]\n",
        "\n",
        "  return quote\n",
        "\n",
        "def bs_to_list(tags):\n",
        "\n",
        "  if type(tags) != list:\n",
        "\n",
        "    tags = tags.find_all('a')\n",
        "  \n",
        "    tag_strs = []\n",
        "    for tag in tags[:-1]:\n",
        "\n",
        "      tag = str(tag)\n",
        "      start_idx = tag.index('\">')\n",
        "      end_idx = tag.index('</')\n",
        "      tag = tag[start_idx + 2:end_idx]\n",
        "      tag_strs.append(tag)\n",
        "\n",
        "    tags = tag_strs\n",
        "\n",
        "  return tags"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86a10b2c",
      "metadata": {
        "id": "86a10b2c"
      },
      "source": [
        "For the tags, we have to a slightly more complicated function since the data is tucked into lists. Firstly, we'll make it a string, and use regex to remove the surrounding brackets, remove non-word characters, and replace all multi-whitespaces with a single space. We'll then render the string as a list again, and return the list. \n",
        "\n",
        "For example, if we input a list like [deep?, wonderous.., love-happy], we would get an output of [deep, wonderous, love, happy]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7baedccc",
      "metadata": {
        "id": "7baedccc"
      },
      "outputs": [],
      "source": [
        "goodreads['quote'] = goodreads['quote'].apply(remove_author)\n",
        "goodreads['quote'] = goodreads['quote'].apply(quotes_cleaning)\n",
        "goodreads['tags'] = goodreads['tags'].apply(bs_to_list)\n",
        "goodreads['tags'] = goodreads['tags'].apply(tags_cleaning)\n",
        "\n",
        "isEmpty = goodreads['tags'].apply(lambda x: '' in x)\n",
        "goodreads['isEmpty'] = isEmpty"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f4e5cf",
      "metadata": {
        "id": "55f4e5cf"
      },
      "source": [
        "Now, having cleaned the dataset more fully, we can see the impact on our data. We've also added an isEmpty column, which marks whether or not the quote has no tags. Since the tags are in a list, empty lists will exist as \"[ ]\" and not as a NaN value.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21f6be24",
      "metadata": {
        "id": "21f6be24"
      },
      "outputs": [],
      "source": [
        "goodreads.head(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "982252e6",
      "metadata": {
        "id": "982252e6"
      },
      "source": [
        "# Reshaping Data for Modeling "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5b9174",
      "metadata": {
        "id": "ce5b9174"
      },
      "source": [
        "Now, while the data is cleaned, we can't really model accurately when our tags are all in a list. Inputting them into our sklearn Pipelines later would not work as we would want, so we have to find a way to reshape the dataframe. Firstly, we'll need to collect the minimum and maximum amount of tags, which we do as follows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "708238d7",
      "metadata": {
        "scrolled": true,
        "id": "708238d7"
      },
      "outputs": [],
      "source": [
        "max_tags = goodreads['tags'].apply(lambda x: len(x)).max()\n",
        "max_tags"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12fc3047",
      "metadata": {
        "id": "12fc3047"
      },
      "source": [
        "So we see that the maximum amount of tags a quote could have would be 48 tags. So, let's generate a function that will make each list of tags equivalent by adding the necessary number of None values to make it to a list of length 48. \n",
        "\n",
        "For example, an input of [life, duck, nature] would yield [life, duck, nature, None, None, None, None, None, None, None, None, ... None, None, None]. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cf95757",
      "metadata": {
        "id": "7cf95757"
      },
      "outputs": [],
      "source": [
        "def pad(tags):\n",
        "\n",
        "  needed = 48 - len(tags)\n",
        "  tags = tags + ([None] * needed)\n",
        "\n",
        "  return tags"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f200c1a6",
      "metadata": {
        "id": "f200c1a6"
      },
      "source": [
        "Now we can apply that function to our Tags column, and use pandas get_dummies() functionality to reshape our dataframe to where each tag is a column, and the column contains 1s or 0s, reflecting whether or not a particular tag is in the quote belonging to that row. \n",
        "\n",
        "Unfortunately, pd.get_dummies() will create some duplicates so we'll groupby and sum to combine the duplicate tag columns. \n",
        "\n",
        "We then combine this dataframe with our original dataframe, and drop our tags columns. We can see the finished result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa43dfb7",
      "metadata": {
        "scrolled": true,
        "id": "aa43dfb7"
      },
      "outputs": [],
      "source": [
        "gr_tags = pd.DataFrame(goodreads['tags'].apply(pad).tolist())\n",
        "gr_tags_oh = pd.get_dummies(gr_tags, prefix = 'tags')\n",
        "gr_tags_oh = gr_tags_oh.groupby(gr_tags_oh.columns, axis = 1).sum()\n",
        "reshaped_gr = pd.concat([goodreads, gr_tags_oh], axis = 1).drop(columns = ['isEmpty', 'tags'])\n",
        "reshaped_gr.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45b11d4a",
      "metadata": {
        "id": "45b11d4a"
      },
      "source": [
        "We can see the distribution of tags as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b538dc55",
      "metadata": {
        "id": "b538dc55"
      },
      "outputs": [],
      "source": [
        "cnts = reshaped_gr.iloc[:, 1:].sum(axis = 1)\n",
        "cnts.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd0454d",
      "metadata": {
        "id": "cbd0454d"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f709dcc",
      "metadata": {
        "id": "6f709dcc"
      },
      "source": [
        "Now, we can begin our modeling. \n",
        "\n",
        "Firstly, we'll get a list of all of our tags. We'll do this by taking all columns besides \"Quote\"\n",
        "\n",
        "Next, we'll use sklearn's train_test_split() function to split our dataset into a training and testing set. We'll split so that our test set is 33% of our dataset size. As we have 100 rows into our data, then we'll have a training set of 67 rows and testing size of 33 rows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2adcc28",
      "metadata": {
        "id": "a2adcc28"
      },
      "outputs": [],
      "source": [
        "gr_tags = reshaped_gr.columns[1:]\n",
        "\n",
        "train, test = train_test_split(reshaped_gr, test_size = 0.25, random_state = 42)\n",
        "\n",
        "x_tr = train.quote\n",
        "x_te = test.quote\n",
        "\n",
        "print(x_tr.shape)\n",
        "print(x_te.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42696382",
      "metadata": {
        "id": "42696382"
      },
      "source": [
        "### Model 1: Decision Tree Classifier\n",
        "\n",
        "Our first model will be using scikit-learn Pipelines. \n",
        "\n",
        "Inside our pipeline, we'll firstly vectorize the input data by converting the quote to their TFIDF formation. This will convert our string Quotes to becoming numerical values for input. Then, we have to consider how we will be handling multiple classes. We'll try with a OneVsRest classifier, because this will allow us to pass in each tag and use an single-class estimator on each tag's train and test data. \n",
        "\n",
        "However, we need to wrap the OneVsRest classifier around an estimator that makes sense for what we are trying to achieve here. We'll use a Decision tree classifier, because the sklearn functionality is pretty simplistic, doesnt require much shaping of the data, and should hopefully set a good basis for our first try. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6d3b9cf",
      "metadata": {
        "id": "f6d3b9cf"
      },
      "outputs": [],
      "source": [
        "dt_classifier = Pipeline([('tfidf', TfidfVectorizer()), ('clf', OneVsRestClassifier(DecisionTreeClassifier()))])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcfc7702",
      "metadata": {
        "id": "dcfc7702"
      },
      "source": [
        "Now, we'll loop through each of the tags in our dataset, train our model on that particular tag, and then append it to a dictionary containg each tag and that tag's associated evaluation score. \n",
        "\n",
        "For our evaluating metric, we'll choose to use f1 scores over accuracy, because if we look at our data, we have an imbalance of tags. Some quotes have several tags, while others only have one or two. As such, using accuracy would likely not work well for this scenario. \n",
        "\n",
        "However, we have multiple classes, so it would not make much sense to get a bunch of f1 scores since each tag would give different results. We can instead collect each tag's precision and recall from when the model's predictions are compared to the actual test data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29223acb",
      "metadata": {
        "id": "29223acb"
      },
      "outputs": [],
      "source": [
        "prec_recs = {}\n",
        "for tag in gr_tags:\n",
        "    \n",
        "    dt_classifier.fit(x_tr, train[tag])\n",
        "    prediction = dt_classifier.predict(x_te)\n",
        "    \n",
        "    precision_recall = precision_recall_fscore_support(test[tag], prediction, average = 'macro')\n",
        "    prec_recs[tag] = precision_recall"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42c71ad3",
      "metadata": {
        "id": "42c71ad3"
      },
      "source": [
        "So now we can calculate the average precision and recall for our tags by looping through our dictionary, summing up the total of both metrics, and dividing by the number of tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5590eb64",
      "metadata": {
        "id": "5590eb64"
      },
      "outputs": [],
      "source": [
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "\n",
        "for key in prec_recs.keys():\n",
        "    \n",
        "    sum_precision += prec_recs[key][0]\n",
        "    sum_recall += prec_recs[key][1]\n",
        "    \n",
        "mean_precision = sum_precision / len(prec_recs.keys())\n",
        "mean_recall = sum_recall / len(prec_recs.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f60216",
      "metadata": {
        "id": "d8f60216"
      },
      "source": [
        "Now we apply the formula of finding an f1 score which is (2 * p * r) / (p + r)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d942fa49",
      "metadata": {
        "id": "d942fa49"
      },
      "outputs": [],
      "source": [
        "average_f1 = (2 * mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
        "average_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6039cf1",
      "metadata": {
        "id": "c6039cf1"
      },
      "source": [
        "So we have an f1 score of about 0.708. F1 scores range from 0 to 1, and the closer they are to 1, the better the model, so we have set up a good baseline for ourselves. But we want to improve on this and make our model better classify our quotes. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b93fd599",
      "metadata": {
        "id": "b93fd599"
      },
      "source": [
        "#### Optimizing Model 1  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c5ea4e",
      "metadata": {
        "id": "68c5ea4e"
      },
      "source": [
        "Now that we have our baseline model, how can we optimize it? \n",
        "\n",
        "There are many concepts we can implement into our Pipeline, both from a text classification standpoint, as well as a sklearn standpoint. \n",
        "\n",
        "The first method we'll implement is getting rid of stop-words. These are words that appear extremely frequently in human language, and give very little value to our model. Removing them can allow our model to focus more strongly on the more important data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41bbe4f3",
      "metadata": {
        "id": "41bbe4f3"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "919b4ad4",
      "metadata": {
        "id": "919b4ad4"
      },
      "source": [
        "Now that we have defined the words to remove, we can try and optimize our other parameters with GridSearchCV. First, we'll need to select what parameters we can optimize. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07867f2b",
      "metadata": {
        "id": "07867f2b"
      },
      "outputs": [],
      "source": [
        "parameters = {\n",
        "    'clf':(DecisionTreeClassifier(),),\n",
        "    'clf__max_depth': [2, 3, 4, 5, 7, 10, 13, 15, 18, None],\n",
        "    'clf__min_samples_split': [2, 3, 5, 7, 10, 15, 20],\n",
        "    'clf__min_samples_leaf': [2, 3, 5, 7, 10, 15, 20]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7342d553",
      "metadata": {
        "id": "7342d553"
      },
      "source": [
        "Now we have created the parameters, we can place that into a GridSearchCV and train it on our data. \n",
        "Let's print out the best parameters we get. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b22152f",
      "metadata": {
        "id": "5b22152f"
      },
      "outputs": [],
      "source": [
        "grids = GridSearchCV(dt_classifier, param_grid = parameters, cv = 3, return_train_score = True)\n",
        "for tag in gr_tags:\n",
        "    grids.fit(x_tr, train[tag])\n",
        "\n",
        "grids.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e25b700",
      "metadata": {
        "id": "9e25b700"
      },
      "source": [
        "Let's now re-run our training, testing, and calculating of precision and recall to calculate a new and hopefully improved average f1 score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb2a078",
      "metadata": {
        "id": "eeb2a078"
      },
      "outputs": [],
      "source": [
        "dt_classifier = Pipeline([('tdidf', TfidfVectorizer(stop_words = stop_words)), ('dtc', DecisionTreeClassifier(max_depth = 2, min_samples_leaf = 2))])\n",
        "\n",
        "prec_recs = {}\n",
        "for tag in gr_tags:\n",
        "    \n",
        "    dt_classifier.fit(x_tr, train[tag])\n",
        "    prediction = dt_classifier.predict(x_te)\n",
        "    \n",
        "    precision_recall = precision_recall_fscore_support(test[tag], prediction, average = 'macro')\n",
        "    prec_recs[tag] = precision_recall\n",
        "\n",
        "sum_precision = 0\n",
        "sum_recall = 0\n",
        "\n",
        "for key in prec_recs.keys():\n",
        "    \n",
        "    sum_precision += prec_recs[key][0]\n",
        "    sum_recall += prec_recs[key][1]\n",
        "    \n",
        "mean_precision = sum_precision / len(prec_recs.keys())\n",
        "mean_recall = sum_recall / len(prec_recs.keys())\n",
        "\n",
        "average_f1 = (2 * mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
        "average_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e679a6a3",
      "metadata": {
        "id": "e679a6a3"
      },
      "source": [
        "So we see a decent improvement from 0.71 --> 0.78, achieved with GridSearchCV and stop_word inclusion to optimize our model. However, we'll take a look at other models / optimizations to see if we can get a heightened score. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84a537a2",
      "metadata": {
        "id": "84a537a2"
      },
      "source": [
        "### Model 2: "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next model we'll try is a Keras Sequential() model with a couple layers. Firstly, we'll already go ahead and remove stopwords from both the quotes and tags, and append these to new columns. "
      ],
      "metadata": {
        "id": "DRIUOH-y8z-z"
      },
      "id": "DRIUOH-y8z-z"
    },
    {
      "cell_type": "code",
      "source": [
        "goodreads = goodreads[goodreads['isEmpty'] == False]\n",
        "goodreads['stop_quote'] = goodreads['quote'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "goodreads['stop_tags'] = goodreads['tags'].apply(lambda x: [z for z in x if z not in stop_words])\n",
        "\n",
        "goodreads.head()"
      ],
      "metadata": {
        "id": "lVdL9EQQZ8cO"
      },
      "id": "lVdL9EQQZ8cO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll shuffle the data using pandas sample() functions, and take only our new stop-word removed quotes and tags columns. Then we'll split training and testing data on an 80/20 split, and get validation data as half the test data.\n",
        "\n",
        "The shapes of the data are as follows:"
      ],
      "metadata": {
        "id": "1Z9QmTDJ9H55"
      },
      "id": "1Z9QmTDJ9H55"
    },
    {
      "cell_type": "code",
      "source": [
        "goodreads_sample = goodreads.sample(frac = 1)\n",
        "goodreads_sample = goodreads_sample[['stop_tags', 'stop_quote']]\n",
        "train, test = train_test_split(goodreads_sample, test_size = 0.2, shuffle = True)\n",
        "val = test.sample(frac=0.5)\n",
        "test.drop(val.index, inplace=True)\n",
        "train.shape, test.shape, val.shape"
      ],
      "metadata": {
        "id": "USBExS3qxJmd"
      },
      "id": "USBExS3qxJmd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll encode the list of strings in the \"stop_tags\" column to shift them from just being Strings to an integer output. We'll accomplish this via keras' constant() and StringLookup() functionality. "
      ],
      "metadata": {
        "id": "B4oMF7cEAqKe"
      },
      "id": "B4oMF7cEAqKe"
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import literal_eval\n",
        "from tensorflow.ragged import constant\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "terms = constant(train[\"stop_tags\"].values)\n",
        "lookup = layers.StringLookup(output_mode=\"multi_hot\")\n",
        "lookup.adapt(terms)"
      ],
      "metadata": {
        "id": "ejX6_BOAy4AL"
      },
      "id": "ejX6_BOAy4AL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to need some information about our quote data, so we'll quickly split them into lists and use pandas describe() methods to generate the info about max lengths, avg lengths, etc."
      ],
      "metadata": {
        "id": "ikEvwdE6A-F-"
      },
      "id": "ikEvwdE6A-F-"
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"stop_quote\"].apply(lambda x: len(x.split(\" \"))).describe()"
      ],
      "metadata": {
        "id": "nOxTfAAy5Uul"
      },
      "id": "nOxTfAAy5Uul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_seqlen = 10\n",
        "batch_size = 6\n",
        "padding_token = \"<pad>\"\n",
        "\n",
        "from tensorflow.data import AUTOTUNE\n",
        "from tensorflow.data import Dataset\n",
        "\n",
        "auto = AUTOTUNE\n",
        "\n",
        "\n",
        "def make_dataset(dataframe, is_train=True):\n",
        "    labels = constant(dataframe[\"stop_tags\"].values)\n",
        "    label_binarized = lookup(labels).numpy()\n",
        "\n",
        "    dataset = Dataset.from_tensor_slices(\n",
        "        (dataframe[\"stop_quote\"].values, label_binarized)\n",
        "    )\n",
        "\n",
        "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
        "    return dataset.batch(batch_size)\n",
        "\n",
        "train_dataset = make_dataset(train, is_train=True)\n",
        "validation_dataset = make_dataset(val, is_train=False)\n",
        "test_dataset = make_dataset(test, is_train=False)"
      ],
      "metadata": {
        "id": "6NBCdZQp6HDo"
      },
      "id": "6NBCdZQp6HDo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = set()\n",
        "train[\"stop_quote\"].str.lower().str.split().apply(vocabulary.update)\n",
        "vocabulary_size = len(vocabulary)\n",
        "print(vocabulary_size)"
      ],
      "metadata": {
        "id": "NAUc4TeW6eW9"
      },
      "id": "NAUc4TeW6eW9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorizer = layers.TextVectorization(max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\")\n",
        "\n",
        "import tensorflow as tf\n",
        "# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n",
        "# training set.\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "train_dataset = train_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
        "validation_dataset = validation_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)\n",
        "test_dataset = test_dataset.map(lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto).prefetch(auto)"
      ],
      "metadata": {
        "id": "HCapfP8E60VO"
      },
      "id": "HCapfP8E60VO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras_model = Sequential()\n",
        "keras_model.add(Dropout(0.2))\n",
        "keras_model.add(Dense(1000, activation = 'relu'))\n",
        "keras_model.add(Dense(500, activation = 'relu'))\n",
        "keras_model.add(Dense(lookup.vocabulary_size(), activation = 'sigmoid'))\n",
        "\n",
        "keras_model.compile(loss=\"binary_crossentropy\", optimizer = 'adam', metrics=['categorical_accuracy'])\n",
        "keras_model.build((None, vocabulary_size))\n",
        "keras_model.summary()"
      ],
      "metadata": {
        "id": "G1Zq4ywP7GB9"
      },
      "id": "G1Zq4ywP7GB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = keras_model.fit(train_dataset, \n",
        "                          validation_data = validation_dataset, \n",
        "                          epochs = 15,\n",
        "                          callbacks = [EarlyStopping(monitor = 'categorical_accuracy', patience = 3)])"
      ],
      "metadata": {
        "id": "utdWTtwS7WXG"
      },
      "id": "utdWTtwS7WXG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = keras_model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "yrAMk_t57Y7G"
      },
      "id": "yrAMk_t57Y7G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label=item)\n",
        "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"loss\")\n",
        "plot_result(\"categorical_accuracy\")"
      ],
      "metadata": {
        "id": "0G3y3g_U8Wvu"
      },
      "id": "0G3y3g_U8Wvu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the unsuccessful nature of using a list of tags, let's try a completely different approach and see if we can make a better result, albeit from a different standpoint. \n",
        "\n",
        "Instead of trying to classify from a multilabel view, what if we just assign one tag to each quote? We saw earlier than many tags are repeated throughout the quotes, so random selection of one would simplify the problem. \n",
        "\n",
        "We'll apply this idea by using numpy's random.choice() method on each list of tags and putting that new tag in a column called 'rand_tag'. "
      ],
      "metadata": {
        "id": "Zha1rzKdBTWY"
      },
      "id": "Zha1rzKdBTWY"
    },
    {
      "cell_type": "code",
      "source": [
        "subset = goodreads[['stop_quote', 'stop_tags']]\n",
        "subset['rand_tag'] = subset['stop_tags'].apply(lambda x: np.random.choice(x))\n",
        "subset.head()"
      ],
      "metadata": {
        "id": "JsfZfIjeiDqc"
      },
      "id": "JsfZfIjeiDqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll encode the tag by looping through the tags and placing them in a dictionary. If the tag exists in the dictionary, then nothing is done; however, if not, then it is added, and the number assigned is the new length of the dictionary. \n",
        "\n",
        "For example, a group of lists like:\n",
        "\n",
        "['life', 'love', 'data']\n",
        "\n",
        "['love', 'joy', 'science'] \n",
        "\n",
        "would generate a dictionary of \n",
        "\n",
        "{'life': 1, 'love': 2, 'data': 3, 'joy': 4, 'science': 5}"
      ],
      "metadata": {
        "id": "WCvQjwfbB3O_"
      },
      "id": "WCvQjwfbB3O_"
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = {}\n",
        "\n",
        "for tag in subset.rand_tag:\n",
        "\n",
        "  if tag not in encoded.keys():\n",
        "\n",
        "    encoded[tag] = len(encoded)\n",
        "\n",
        "subset['encoded_tag'] = subset.rand_tag.map(encoded)"
      ],
      "metadata": {
        "id": "i5NwODewwuLu"
      },
      "id": "i5NwODewwuLu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll shuffle the data again and reset the index. "
      ],
      "metadata": {
        "id": "MgN9LkHBClm4"
      },
      "id": "MgN9LkHBClm4"
    },
    {
      "cell_type": "code",
      "source": [
        "subset = subset.sample(frac = 1).reset_index(drop = True)\n",
        "subset.head()"
      ],
      "metadata": {
        "id": "Ipv_1qfWxdd-"
      },
      "id": "Ipv_1qfWxdd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we'll onehot encode the tags via keras' to_categorical() function. This will take a Series of integers and convert it to arrays of 1 (present) and 0s (not present). "
      ],
      "metadata": {
        "id": "P2hJcpbIDvdV"
      },
      "id": "P2hJcpbIDvdV"
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoded = to_categorical(subset.encoded_tag)\n",
        "onehot_encoded"
      ],
      "metadata": {
        "id": "oCPuME83ygYC"
      },
      "id": "oCPuME83ygYC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use a AutoTokenizer to help tokenize our quotes later on, as well as a pretrained BERT model. "
      ],
      "metadata": {
        "id": "yNntqD_uEJWo"
      },
      "id": "yNntqD_uEJWo"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
        "bert = TFBertModel.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "hIDgRG5rzT6q"
      },
      "id": "hIDgRG5rzT6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll split into training and testing data, with a split of 80/20 on our stop_quote and encoded_tag columns. \n",
        "\n",
        "We'll again find out information to inform our model via pandas describe() functionality. "
      ],
      "metadata": {
        "id": "GgKKCKLCGf5Y"
      },
      "id": "GgKKCKLCGf5Y"
    },
    {
      "cell_type": "code",
      "source": [
        "data = subset[['stop_quote', 'encoded_tag']]\n",
        "train, test = train_test_split(data, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "ulYA-F6xz0a8"
      },
      "id": "ulYA-F6xz0a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset[\"stop_quote\"].apply(lambda x: len(x.split(\" \"))).describe()"
      ],
      "metadata": {
        "id": "jvftf0Xg0iSG"
      },
      "id": "jvftf0Xg0iSG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll develop our x values from our training and testing datasets and tokenize them via the tokenizer initialization. \n",
        "\n",
        "We have several parameters to use:\n",
        "- add_special_tokens is True to add tokens to the start and end of our tokenized quotes. \n",
        "- max_length is based off the information we saw earlier. This informs why we have truncation = True\n",
        "- return_attention_mask = True. This informs the model not to pay attention to the special padding tokens when it reads the sentence. "
      ],
      "metadata": {
        "id": "FjsCi3thHJB_"
      },
      "id": "FjsCi3thHJB_"
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tokenizer(\n",
        "    text=train['stop_quote'].tolist(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=9,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)\n",
        "x_test = tokenizer(\n",
        "    text=test['stop_quote'].tolist(),\n",
        "    add_special_tokens=True,\n",
        "    max_length=9,\n",
        "    truncation=True,\n",
        "    padding=True, \n",
        "    return_tensors='tf',\n",
        "    return_token_type_ids = False,\n",
        "    return_attention_mask = True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "y2QI43-m0LlZ"
      },
      "id": "y2QI43-m0LlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can generate our y values from our training and testing datasets."
      ],
      "metadata": {
        "id": "A6yj7zV4KNQa"
      },
      "id": "A6yj7zV4KNQa"
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['encoded_tag']\n",
        "y_test = test['encoded_tag']"
      ],
      "metadata": {
        "id": "Ws5Rbsnv0nhX"
      },
      "id": "Ws5Rbsnv0nhX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also get our input_ids and attention_masks. The former is a list of tokens that we are going to feed into our BERT model to be read and interpreted. The latter is a list of indices that tells the model to use and not use certain ids that correspond to special_tokens. \n"
      ],
      "metadata": {
        "id": "NzN1P4CMKVX6"
      },
      "id": "NzN1P4CMKVX6"
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = x_train['input_ids']\n",
        "attention_mask = x_train['attention_mask']"
      ],
      "metadata": {
        "id": "ycq90snG0sy4"
      },
      "id": "ycq90snG0sy4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Dense"
      ],
      "metadata": {
        "id": "wyHLK0vK0wER"
      },
      "id": "wyHLK0vK0wER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we begin building our model. \n",
        "\n",
        "Firstly, we need to "
      ],
      "metadata": {
        "id": "daMiuhhxLNlG"
      },
      "id": "daMiuhhxLNlG"
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 9\n",
        "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
        "embeddings = bert(input_ids,attention_mask = input_mask)[0] \n",
        "out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
        "out = Dense(256, activation='relu')(out)\n",
        "out = tf.keras.layers.Dropout(0.1)(out)\n",
        "out = Dense(64, activation = 'relu')(out)\n",
        "y = Dense(1, activation = 'sigmoid')(out)\n",
        "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
        "model.layers[2].trainable = True"
      ],
      "metadata": {
        "id": "pC4f2C0909QP"
      },
      "id": "pC4f2C0909QP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "tHo5G6OV6OXb"
      },
      "id": "tHo5G6OV6OXb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(\n",
        "    learning_rate=5e-05, # this learning rate is for bert model , taken from huggingface website \n",
        "    epsilon=1e-08,\n",
        "    decay=0.01,\n",
        "    clipnorm=1.0)\n",
        "# Set loss and metrics\n",
        "loss = CategoricalCrossentropy(from_logits = True)\n",
        "metric = CategoricalAccuracy('balanced_accuracy'),\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer = optimizer,\n",
        "    loss = loss, \n",
        "    metrics = [Precision(), Recall()])#metric)"
      ],
      "metadata": {
        "id": "RTY_BW6g1MaN"
      },
      "id": "RTY_BW6g1MaN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_history = model.fit(\n",
        "    x = {'input_ids': x_train['input_ids'],'attention_mask': x_train['attention_mask']} ,\n",
        "    y = y_train,\n",
        "    validation_data = (\n",
        "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']}, y_test\n",
        "    ), epochs=1, batch_size=32\n",
        ")"
      ],
      "metadata": {
        "id": "ouUZpySm1QbH"
      },
      "id": "ouUZpySm1QbH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_info = model.evaluate(\n",
        "    {'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']},\n",
        "    y_test\n",
        ")"
      ],
      "metadata": {
        "id": "6BRdnWkC9j_I"
      },
      "id": "6BRdnWkC9j_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can finally calculate our final F1 score, and we see the major improvement with this model that we achieved. "
      ],
      "metadata": {
        "id": "s5UCiEikOBIT"
      },
      "id": "s5UCiEikOBIT"
    },
    {
      "cell_type": "code",
      "source": [
        "p = test_info.history['precision']\n",
        "r = test_info.history['recall']\n",
        "\n",
        "f1_score = (2 * p * r) / (p + r)\n",
        "f1_score"
      ],
      "metadata": {
        "id": "4JUI0XCV1TWW"
      },
      "id": "4JUI0XCV1TWW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Copy of Project Tester.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}